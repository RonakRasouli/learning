{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "skilled-sapphire",
   "metadata": {},
   "source": [
    "# DeepQ Learning: Mountain Car example in Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surface-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roni/opt/anaconda3/envs/assign/lib/python3.9/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/Users/roni/opt/anaconda3/envs/assign/lib/python3.9/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/Users/roni/opt/anaconda3/envs/assign/lib/python3.9/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/Users/roni/opt/anaconda3/envs/assign/lib/python3.9/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "/Users/roni/opt/anaconda3/envs/assign/lib/python3.9/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "/Users/roni/opt/anaconda3/envs/assign/lib/python3.9/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from rl.core import Processor\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy,LinearAnnealedPolicy,BoltzmannQPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7397dd9",
   "metadata": {},
   "source": [
    "# 1. Import the MountainCar environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "demographic-cannon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of state =  2\n",
      "No of actions = 3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "print(\"Shape of state = \", env.observation_space.shape[0])\n",
    "print(\"No of actions =\", env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-joseph",
   "metadata": {},
   "source": [
    "# 2.  Try some random moves to provide a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simplified-grocery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0> 2> 1> 2> 2> 1> 0> 2> 2> 1> 0> 2> 1> 1> 0> 1> 2> 2> 1> 1> 0> 0> 0> 0> 2> 1> 0> 0> 1> 2> 0> 1> 1> 1> 2> 2> 2> 2> 2> 2> 1> 2> 0> 2> 0> 1> 2> 0> 2> 1> 0> 0> 0> 0> 1> 2> 0> 0> 2> 2> 0> 0> 0> 0> 2> 2> 0> 2> 1> 1> 1> 1> 2> 1> 0> 0> 2> 1> 2> 1> 1> 0> 0> 1> 2> 2> 2> 2> 2> 0> 0> 1> 2> 1> 2> 0> 1> 1> 0> 1> 1> 1> 0> 0> 2> 2> 1> 0> 2> 2> 2> 1> 2> 2> 0> 1> 1> 1> 0> 1> 0> 1> 0> 0> 0> 0> 2> 1> 0> 2> 0> 0> 2> 1> 0> 2> 2> 0> 2> 2> 2> 2> 0> 1> 1> 1> 0> 0> 2> 2> 2> 2> 2> 1> 0> 0> 0> 1> 1> 0> 2> 0> 1> 2> 0> 1> 2> 2> 2> 1> 0> 2> 0> 0> 2> 0> 1> 1> 0> 1> 0> 2> 1> 0> 2> 1> 2> 2> 1> 0> 0> 0> 0> 2> 1> 0> 1> 1> 1> 1\n",
      " Episode 1 Total Reward = -200.0\n",
      "> 0> 2> 2> 2> 2> 2> 2> 2> 0> 0> 0> 1> 2> 0> 2> 1> 1> 2> 2> 1> 0> 2> 2> 1> 0> 0> 1> 0> 1> 1> 1> 2> 1> 1> 2> 0> 1> 0> 0> 2> 0> 0> 0> 1> 1> 0> 0> 0> 0> 0> 2> 2> 2> 0> 1> 0> 1> 1> 0> 0> 2> 0> 0> 2> 0> 0> 2> 1> 0> 0> 1> 1> 0> 2> 0> 1> 1> 2> 0> 1> 2> 1> 2> 1> 0> 1> 1> 2> 1> 1> 0> 1> 2> 1> 2> 2> 1> 0> 0> 1> 1> 0> 2> 0> 0> 2> 1> 1> 1> 2> 1> 1> 2> 1> 1> 1> 1> 0> 2> 1> 2> 1> 1> 0> 2> 0> 0> 2> 2> 1> 1> 1> 2> 0> 0> 1> 0> 2> 1> 1> 1> 0> 2> 2> 0> 2> 0> 0> 1> 0> 0> 1> 0> 0> 2> 2> 0> 1> 0> 1> 0> 0> 2> 0> 1> 2> 1> 1> 1> 2> 1> 2> 1> 1> 0> 0> 1> 2> 0> 2> 2> 0> 0> 1> 0> 2> 0> 0> 1> 2> 1> 1> 0> 0> 1> 1> 1> 0> 1> 0\n",
      " Episode 2 Total Reward = -200.0\n",
      "> 1> 2> 0> 1> 2> 2> 1> 1> 0> 1> 0> 2> 2> 0> 2> 0> 0> 0> 0> 0> 2> 0> 1> 1> 1> 2> 1> 0> 2> 1> 0> 2> 1> 0> 2> 2> 1> 0> 1> 1> 0> 1> 1> 2> 0> 0> 0> 1> 2> 1> 2> 0> 0> 1> 2> 0> 2> 2> 0> 2> 2> 0> 1> 0> 1> 0> 1> 1> 1> 2> 0> 2> 0> 0> 1> 0> 0> 1> 1> 0> 2> 1> 2> 2> 1> 1> 2> 1> 1> 0> 0> 2> 0> 1> 1> 1> 1> 2> 0> 2> 0> 0> 0> 1> 0> 0> 0> 1> 2> 0> 2> 2> 0> 2> 1> 1> 0> 0> 2> 2> 0> 0> 0> 1> 2> 2> 1> 2> 1> 1> 2> 1> 0> 1> 0> 2> 0> 1> 0> 0> 1> 2> 2> 1> 1> 2> 1> 1> 0> 1> 1> 1> 2> 0> 1> 2> 0> 1> 0> 2> 2> 2> 1> 0> 1> 1> 1> 0> 2> 0> 1> 2> 1> 0> 1> 2> 2> 2> 1> 2> 1> 0> 1> 1> 1> 0> 1> 2> 2> 0> 0> 2> 2> 0> 1> 2> 1> 2> 1> 1\n",
      " Episode 3 Total Reward = -200.0\n",
      "> 2> 0> 1> 1> 1> 0> 2> 0> 1> 0> 0> 0> 2> 2> 2> 2> 1> 1> 0> 0> 1> 1> 2> 2> 0> 2> 1> 2> 2> 1> 2> 0> 1> 2> 1> 1> 0> 0> 0> 0> 1> 1> 1> 2> 2> 0> 0> 0> 0> 2> 2> 1> 0> 0> 1> 2> 1> 2> 0> 0> 1> 2> 1> 0> 0> 1> 1> 1> 1> 2> 1> 2> 1> 2> 1> 1> 0> 2> 1> 0> 2> 0> 2> 0> 2> 1> 0> 1> 0> 2> 0> 1> 0> 0> 0> 2> 1> 0> 0> 2> 2> 1> 1> 0> 2> 1> 2> 2> 1> 0> 2> 2> 0> 2> 2> 0> 1> 1> 2> 0> 0> 0> 2> 1> 0> 0> 2> 2> 2> 0> 1> 2> 1> 2> 0> 0> 1> 2> 2> 1> 2> 0> 2> 0> 0> 1> 2> 2> 0> 2> 0> 1> 1> 1> 2> 2> 0> 2> 2> 2> 2> 1> 0> 2> 1> 2> 0> 0> 0> 1> 0> 2> 0> 1> 0> 1> 1> 2> 1> 0> 1> 2> 2> 2> 0> 2> 1> 2> 1> 1> 0> 1> 1> 0> 1> 0> 0> 2> 2> 0\n",
      " Episode 4 Total Reward = -200.0\n",
      "> 0> 1> 2> 0> 0> 0> 2> 2> 0> 0> 0> 0> 2> 2> 1> 0> 0> 0> 2> 2> 1> 2> 0> 1> 1> 2> 1> 0> 2> 2> 2> 0> 1> 2> 1> 2> 2> 1> 2> 2> 0> 0> 0> 2> 2> 1> 1> 1> 1> 2> 0> 1> 1> 2> 0> 1> 2> 1> 2> 2> 2> 1> 1> 2> 0> 2> 0> 2> 2> 0> 0> 0> 1> 0> 1> 0> 0> 1> 1> 1> 2> 1> 0> 0> 2> 0> 1> 1> 2> 1> 0> 2> 0> 1> 1> 0> 2> 0> 2> 0> 2> 1> 0> 0> 2> 1> 0> 1> 2> 1> 1> 1> 0> 1> 0> 2> 2> 0> 0> 0> 2> 1> 2> 0> 1> 2> 2> 2> 0> 0> 1> 0> 1> 2> 0> 2> 1> 2> 0> 0> 1> 0> 1> 1> 2> 2> 1> 1> 2> 2> 2> 0> 2> 2> 0> 1> 1> 2> 1> 0> 1> 0> 0> 1> 1> 0> 0> 2> 2> 2> 1> 0> 2> 1> 1> 0> 1> 0> 2> 0> 0> 1> 2> 0> 0> 2> 0> 2> 1> 0> 0> 0> 2> 2> 2> 1> 0> 1> 2> 1\n",
      " Episode 5 Total Reward = -200.0\n",
      "Average Reward = -200.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "cumulative_reward = 0\n",
    "\n",
    "for e in range(1,episodes+1):\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        print(\">\",action, end='')\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(f'\\n Episode {e} Total Reward = {total_reward}')\n",
    "    cumulative_reward += total_reward\n",
    "    \n",
    "average_reward = cumulative_reward/episodes \n",
    "print('Average Reward =',average_reward )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-penny",
   "metadata": {},
   "source": [
    "# 3. Use the DQN model for the MountainCar example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "disabled-priest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "18\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 18)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                456       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 48)                1200      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 147       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,403\n",
      "Trainable params: 2,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_agent(states, actions):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape = (1, states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(env.observation_space.shape[0])\n",
    "\n",
    "# no_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "#                 np.array([10, 100])\n",
    "# no_states = np.round(no_states, 0).astype(int) + 1\n",
    "no_states = 18\n",
    "print(no_states)\n",
    "model = create_agent( no_states , env.action_space.n)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6909919c",
   "metadata": {},
   "source": [
    "# 4. Defining the connection between  the environment and agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9629708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, change state to hot encoding encoding and other \n",
    "class MountainCarProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        #print(observation)\n",
    "        one_hot = np.zeros(18)\n",
    "\n",
    "        # identify the index of observation state in x-axis:\n",
    "        i = int(np.round(( observation[0] + 1.2 / 0.1 )))\n",
    "        one_hot[i] = 1\n",
    "        return one_hot\n",
    "        \n",
    "    def process_reward(self, reward):\n",
    "        if (env.state[0] >= 0.5):\n",
    "            new_reward = 2\n",
    "        else: \n",
    "            new_reward = (env.state[0] + 1.2) / 1.8 - 1\n",
    "        return new_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loose-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "memory = SequentialMemory(limit= 100000, window_length= 1 )\n",
    "\n",
    "processor = MountainCarProcessor()\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', \n",
    "                             value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions= env.action_space.n, processor = processor ,\n",
    "               memory= memory, nb_steps_warmup=100, gamma=0.99, policy=policy, \n",
    "               enable_double_dqn= True,target_model_update= 1e-3 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397b474",
   "metadata": {},
   "source": [
    "# 5. Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fifth-pocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roni/opt/anaconda3/envs/assign/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n",
      "2022-05-30 12:57:16.642014: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-30 12:57:16.654502: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   78/10000 [..............................] - ETA: 6s - reward: -0.6278   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roni/opt/anaconda3/envs/assign/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 60s 6ms/step - reward: -0.6197\n",
      "50 episodes - episode_reward: -123.947 [-129.383, -118.710] - loss: 0.026 - mae: 2.079 - mean_q: -3.065 - mean_eps: 0.545\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -0.6226\n",
      "50 episodes - episode_reward: -124.515 [-131.287, -113.853] - loss: 0.166 - mae: 5.479 - mean_q: -8.110 - mean_eps: 0.100\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -0.6216\n",
      "50 episodes - episode_reward: -124.314 [-132.508, -111.625] - loss: 0.412 - mae: 8.536 - mean_q: -12.653 - mean_eps: 0.100\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -0.5971\n",
      "50 episodes - episode_reward: -119.040 [-127.398, -106.605] - loss: 0.723 - mae: 11.146 - mean_q: -16.535 - mean_eps: 0.100\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -0.5838\n",
      "50 episodes - episode_reward: -116.783 [-127.294, -109.972] - loss: 0.981 - mae: 13.284 - mean_q: -19.720 - mean_eps: 0.100\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: -0.5821\n",
      "50 episodes - episode_reward: -116.371 [-126.160, -104.230] - loss: 1.247 - mae: 14.916 - mean_q: -22.153 - mean_eps: 0.100\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -0.5696\n",
      "50 episodes - episode_reward: -113.923 [-119.997, -107.622] - loss: 1.511 - mae: 16.500 - mean_q: -24.502 - mean_eps: 0.100\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: -0.5700\n",
      "50 episodes - episode_reward: -114.032 [-124.123, -108.973] - loss: 1.748 - mae: 17.800 - mean_q: -26.431 - mean_eps: 0.100\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 70s 7ms/step - reward: -0.5616\n",
      "50 episodes - episode_reward: -112.329 [-121.215, -108.958] - loss: 2.095 - mae: 18.808 - mean_q: -27.917 - mean_eps: 0.100\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: -0.5551\n",
      "done, took 648.292 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn.compile(RMSprop(lr=1e-3), metrics=['mae'])\n",
    "res_train = dqn.fit(env, nb_steps=100000, visualize=False, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c2c488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-117.6267274810171\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.average(res_train.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c0da8",
   "metadata": {},
   "source": [
    "# 6. Test the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "governing-shooting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -108.514, steps: 200\n",
      "Episode 2: reward: -109.238, steps: 200\n",
      "Episode 3: reward: -108.856, steps: 200\n",
      "Episode 4: reward: -107.750, steps: 200\n",
      "Episode 5: reward: -109.415, steps: 200\n",
      "Episode 6: reward: -108.309, steps: 200\n",
      "Episode 7: reward: -109.122, steps: 200\n",
      "Episode 8: reward: -108.476, steps: 200\n",
      "Episode 9: reward: -107.550, steps: 200\n",
      "Episode 10: reward: -107.893, steps: 200\n",
      "-108.51229085464233\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "res = dqn.test(env, nb_episodes= 10 , visualize=False)\n",
    "print(np.average(res.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f37da2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a9c3fe1cae09371a9d2961a7940d2439703fba8ec2b3b46c9d013595f891ce5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('assign')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
